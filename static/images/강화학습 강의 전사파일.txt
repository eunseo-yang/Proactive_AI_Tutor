인공 지능, 머신러닝, 심층 신경망이라는 용어는

로봇이 생각하고 진화하는 존재로 거듭나는

미래를 떠올리게 만듭니다.

이번 동영상에서는 강화 학습에 대해 알아보겠습니다.

줄여서 RL이라고도 부르죠.

강화 학습은 머신러닝의 한 종류로,

정말 어려운 제어 문제를

해결할 수 있는 가능성을 지니고 있습니다.

인공지능 회사인 Deep Mind가 만든

'알파고'라는 프로그램을 들어보셨을 겁니다.

알파고는 강화 학습을 활용해

세계 최고의 바둑 기사를 이긴 AI입니다.

이 회사는 최근에 스타크래프트2를

평정할 준비가 된 '알파스타' 탄생시켰죠.

자연스럽게 이런 생각이 드실 겁니다.

‘AI가 그 모든 일을 해낸다면 강화 학습을 사용하여

로봇을 제어하거나 데이터 센터를 냉각하거나

드론을 매우 역동적인 난류에서도

안정화할 수 있지 않을까?’

그럼 이제부터 한번 살펴보겠습니다.

그 전에, 잠깐 주의할 점을 말씀드리죠.

오늘은 강화 학습의 모든 것을

알아보는 시간은 아닙니다.

저는 여러분에게 모든 것을

설명할 자격도 없습니다.

대신 전통적으로 교육받은 제어 엔지니어의 관점에서

이 주제를 소개하고, 제어 이론과

실제로 중복되는 부분이 많다는 점을 보여드리려고 합니다.

이 시리즈가 끝날 즈음에는

여러분도 강화 학습의 정의나 제어 문제를 해결할 때

강화 학습을 고려해야 하는 이유,

강화 학습 문제를 설정하고 해결하는 방법,

기존의 제어 방식과 비교할 때

강화 학습이 지니는 장단점에 대해

설명하실 수 있을 것입니다.

이 시리즈의 목적을 짚어드렸으니,

이제 본격적으로 시작해 보겠습니다.

저는 Brian입니다.

MATLAB Tech Talk에 오신 것을 환영합니다.

먼저 전통적인 제어 방식의 관점에서

보행 로봇을 만드는 것이

얼마나 복잡한지 따져 보겠습니다.

우선 주변 환경을 보고

이미지 특징을 추출하여 장애물의 위치나

지역을 신호로 변환하기 위해 카메라를 사용할 것입니다.

카메라를 통한 관측값을 다른 센서와 결합해

상태 추정을 완성할 수 있겠죠.

그런 다음 플랜트 및 환경 모델을 함께 사용하여

제어 시스템을 설계할 수 있습니다.

이런 제어 시스템은 서로 상호작용하는

여러 제어 루프로 구성될 가능성이 큽니다.

예를 들자면, 다리 궤적 또는 로봇 몸 궤적을 관리하는

낮은 수준의 모터 제어기와 높은 수준의 제어기가 있겠죠.

균형이나 공칭 외 동작을 관리하는

더 높은 수준의 제어기가 있을 수도 있습니다.

불확실한 환경에서 모든 것이 함께 작동해야만

'보행'이라는 복잡한 움직임을 만들어낼 수 있습니다.

꽤나 어려운 문제죠.

이 모든 복잡성을 제쳐 두고,

단순하게 관측값을 가져와 낮은 수준의

모터 명령으로 직접 출력하는

하나의 블랙박스가 있다고 해 봅시다.

우리가 전지전능하다면,

편안하게 앉아서 로봇이 걷게 만드는 함수를 설계할 수 있겠죠.

설계 과정에서 필요한 내부 단계에는

하나도 신경 쓰지 않고요.

하지만 우리는 전지전능하지 않으므로

머신러닝이 필요한 겁니다.

일반적으로 머신러닝은 세 가지 범주로 세분화할 수 있습니다.

비지도 학습, 지도 학습, 강화 학습이 바로 그것이죠.

비지도 학습은 분류되지 않았거나

레이블링 되지 않은 숨겨진 구조체 및

데이터셋 또는 패턴을 찾는 데 사용됩니다.

예를 들어, 동물 10만 마리의 신체적 특성

또는 사회적 경향 같은 정보를 수집했다고 하면

비지도 학습을 사용하여 동물을 그룹화하거나

유사한 특징에 따라 군집화할 수 있습니다.

동물을 포유류, 조류 등 분명한 기준으로 그룹화할 수도 있고,

명백하지 않은 패턴별로 그룹화하여

여러분이 미처 알지 못했던 신체적 특징과

사회적 행동 사이의 상관관계를 찾아낼 수도 있습니다.

반면에 지도 학습은 주어진 입력에 레이블을 적용하도록

컴퓨터를 학습시킨다는 점에서 미묘한 차이가 있습니다.

예를 들어, 동물의 특징에 대한 데이터셋의

열 하나가 종이라고 가정해 보겠습니다.

그러면 종을 레이블로, 나머지 데이터를

수학적 모델에 대한 입력으로 처리할 수 있습니다.

그런 다음 지도 학습을 사용하여

동물의 특징들을 한 번에 하나씩 입력하고,

모델이 종을 추측하도록 한 다음,

그 추측이 맞는지 아닌지에 따라 체계적으로 모델을 수정하여

모델이 동물의 특징들을 세트별로 정확하게

레이블링 하도록 학습시킬 수 있습니다.

만약 우리가 신뢰할 수 있는 모델을 얻을 수 있을 만큼

학습 데이터가 충분히 쌓였다면,

새로운 동물에 대한 특징도 입력하여 전송할 수 있습니다.

우리가 레이블링 하지 않아도 모델이 알아서

그 동물을 가장 가까운 종으로 레이블링할 것입니다.

지도 학습은 많은 사람에게 익숙한

머신러닝의 일종일 겁니다.

컴퓨터는 지도 학습을 통해

여러분 사진 속의 고양이나 친구를 인식하니까요.

이미지를 레이블링하는 것은

동물 특징의 데이터셋을 레이블링하는 것과

근본적으로 동일한 문제입니다.

여러 학습 이미지를 모델에 입력한 다음

모델이 정답을 찾을 때까지 정확하게 추측했는지

아닌지에 따라 모델을 조정하는 것이죠.

차이가 있다면 이미지의 입력 데이터가 픽셀 강도를 나타내는

숫자의 스트림일 뿐이므로 해당 데이터가

고양이와 어떤 관련이 있는지 이해하기란

그리 간단하지 않다는 점입니다.

이게 바로 딥러닝이 잘하는 것입니다.

모델을 심층 신경망으로 표현하면

수천 개의 숫자를 효율적으로 입력하고,

학습 과정에서 모델을 조정하여

수많은 픽셀 강도 내에서 특징을 식별하고,

궁극적으로는 레이블링을 정확하게 해낼 수 있습니다.

강화 학습은 이와 완전히 다릅니다.

정적 데이터셋과 함께 작동하는

다른 두 가지 학습 프레임워크와 달리,

RL은 동적 환경에서 작동합니다.

RL은 데이터를 클러스터링하거나 레이블링하는 것이 아니라

최적의 결과를 생성하는 최상의 행동 시퀀스를 찾습니다.

여기서 '최적'이란 가장 많은 보상을 얻는다는 의미입니다.

이러한 작업은 에이전트라는 소프트웨어가 환경을 탐색하고,

환경과 상호작용하며, 환경으로부터

학습하게 함으로써 이루어집니다.

에이전트는 환경에 영향을 주는 행동을 취해

환경의 상태를 변경할 수 있으며,

이에 따라 환경은 해당 행동에 대한 보상을 생성합니다.

에이전트는 이 정보를 토대로

나중에 수행할 행동을 조정할 수 있습니다.

과정을 통해 배우는 것이죠.

사람은 비록 소프트웨어가 아니지만

본질적으로는 소프트웨어 에이전트가

강화 학습 프레임워크를 통해 학습하는 것과

같은 방식으로 학습합니다.

여러분을 에이전트라고 생각하면,

여러분 주변의 세계는 여러분이 상호작용하고

상태를 관측하며 보상을 수집할 수 있는 환경이 되는 거죠.

좋은 행동을 취하면 환경으로부터 보상을 받습니다.

예를 들어 대학에 가는 게 좋은 행동이라고 치면

여러분은 직업을 갖게 되죠. 이게 상태입니다.

돈을 잘 버는 직업은 좋은 보상이 됩니다.

또는 길을 건너기 전에 양쪽을 다 봅니다.

이게 행동이죠.

이제 반대편에 있는 상태에 도달했습니다.

그 과정에서 사고가 나지 않았죠.

이게 보상입니다.

반대로 나쁜 행동을 취하는 경우에는

적은 보상 혹은 부정적인 보상을 받습니다.

예를 들면 시험 전에 밤을 새우는 것이 행동입니다.

피곤함을 느끼는 게 여러분의 상태죠.

결과적으로 안 좋은 성적이라는 보상을 받게 됩니다.

에이전트 안에는 상태 관측값이라는 입력을 가져온 다음,

이러한 관측값을 행동이라는 출력에 매핑하는 두뇌가 있습니다.

RL 명명법에서는 이 매핑을 정책이라고 합니다.

정책은 일련의 관측값을 토대로

어떤 행동을 취해야 할지를 결정합니다.

지도 학습과 마찬가지로 정책을

심층 신경망으로 나타낼 수 있는데,

나중에 보여드리겠지만,

에이전트가 수천 개의 상태를 동시에 입력하면서도

여전히 의미 있는 행동을 도출해내는 것을

볼 수 있습니다.

여기서 심층 강화 학습이라는 용어가 등장합니다.

보행 로봇의 예제에서 관측값은

카메라 센서의 수천 픽셀 및

모든 관절의 상태일 수 있습니다.

정책은 이러한 모든 관측값을 가져와

액추에이터 명령을 출력합니다.

로봇이 똑바로 서서 계속 걷는다면

환경은 보상을 생성하고 에이전트에게 액추에이터 명령의

매우 특정한 조합이 얼마나 효과적이었는지 알려줄 것입니다.

물론 최상의 행동을 수행하기 위한

정책이 올바르게 매핑되지 않거나

환경이 조금씩 변화하여 매핑이

더 이상 적합하지 않을 수도 있습니다.

여기서 바로 강화 학습 알고리즘이 필요합니다.

이 알고리즘은 취해진 행동,

환경으로부터 얻은 관측값,

수집된 보상을 바탕으로 정책을 바꿉니다.

이러한 방식으로 환경과 상호작용하는 동안

전체 에이전트의 목표는 강화 학습 알고리즘을 사용하여

정책을 수정하는 것이 됩니다.

이를 통해 결국은 어떤 상태에서든

장기적으로 가장 많은 보상을 얻는 데

가장 유리한 행동을 취하게 되죠.

예를 들어, 시험 때 피곤해서 나쁜 성적을 받았다면

그 경험에서 배워서 다음 시험 전에는

늦게까지 깨어 있지 않도록

정책을 조정하는 것입니다.

강화 학습은 본질적으로 최적화 문제지만,

강화 학습을 다른 최적화 기법과

차별화하는 몇 가지 흥미로운 개념들이 있습니다.

첫 번째는 '가치'라는 개념입니다.

보상은 특정 상태에 있을 때

제공되는 순간적인 이익인 반면,

가치는 에이전트가 해당 상태에서는

물론 미래에도 얻을 것으로

기대할 수 있는 총 보상을 의미합니다.

보상을 평가하기보다는 상태 가치를 평가하는 편이

에이전트가 단기적인 이익 대신 시간에 따라

가장 많은 보상을 받을 행동을

선택하도록 하는 데 도움이 되죠.

예를 들어, 에이전트가 이러한 상황에 처해 있고

3단계 내에 가장 많은 보상을

수집하려고 한다고 상상해 봅시다.

만약 에이전트가 각 행동에 대한 보상만 직접 본다면,

우선 왼쪽으로 가서 더 높은 보상을 얻은 다음

오른쪽으로 갔다가 다시 왼쪽으로 가서

최종적으로 +1을 수집할 것입니다.

그러나 에이전트가 상태 가치를 추정할 수 있다면

오른쪽으로 가는 것이 왼쪽으로 가는 것보다

가치가 더 높다는 사실을 알게 될 것이고,

최종적으로는 +8의 보상을 받게 될 것입니다.

물론 미래에 높은 보상을 수집할 수 있다는

약속이 있다 해도

여전히 그 행동이 최선이 아닌 경우도 많습니다.

이를 설명하는 근거는 적어도 두 가지가 있습니다.

첫째, 금융 시장과 마찬가지로

지금 가진 돈의 가치가 1년 후 가지고 있는

더 많은 액수의 돈보다 가치가 높을 수 있습니다.

둘째, 미래로 갈수록 여러분이 예측한 보상은

신뢰성이 떨어지게 됩니다.

따라서 여러분이 미래에 도달할 때쯤에는

높은 보상이 존재하지 않을 수도 있습니다.

이러한 두 경우 모두 가치를 추정할 때

조금 근시안적인 편이 더 유리합니다.

또한 RL에서는 더 먼 미래에 받게 될 보상일수록

더 큰 할인을 적용하여

이러한 문제를 통제할 수 있습니다.

강화 학습의 또 다른 중요한 측면은

환경과 상호작용할 때 적용되는

탐색(exploration)과 이용(exploitation)이라는 개념이

상충 관계라는 점입니다.

이는 가장 많은 보상을 받는 것으로 알고 있는 것과

아직 경험하지 않은 환경 영역을

탐색하는 것 사이의 상충을 의미하죠.

예를 들어, 에이전트가 바로 인접한

두 가지 보상만 알고 있다고 가정해 보겠습니다.

에이전트가 환경을 이용하는

탐욕스러운 접근 방식을 취한다면

자신이 알고 있는 보상 중 최고만 노릴 것이므로

1을 수집하기 위해 왼쪽으로 향할 것입니다.

하지만 간혹 에이전트가 보상을

적게 수집할 위험을 감수하더라도

상태 공간을 탐색하게 한다면

에이전트는 가치 함수를 더 많이 채울 수 있으며

이전에 알지 못했던 더 큰 보상을 찾을 가능성도 열립니다.

이는 인간이 일반적으로 학습하는

과정의 일부이기도 합니다.

간단한 예로 어떤 레스토랑에서

먹을지 결정한다고 칩시다.

기존 지식을 이용하여

좋아하는 식당을 선택하시겠습니까?

아니면 전에 가본 적이 없는 레스토랑을

모험적으로 탐색하여 기존의 지식을 넓히시겠습니까?

새로운 레스토랑을 시도하면

여러분이 좋아할 만한

새로운 장소를 찾을 기회를 얻는 반면

여러분 마음에 들지 않는

식사를 할 가능성도 늘어납니다.

탐색과 이용 사이에서 완벽한 균형을 맞추는 것은 어렵지만,

적어도 RL 알고리즘은 이러한 균형을

유지하는 단순한 방법을 제공합니다.

RL의 목표와 엔지니어가

제어 시스템을 설계할 때의

목표가 전혀 달라 보인다고 느끼실지도 모르겠습니다.

하지만 사실 거의 똑같은 문제입니다.

우리는 플랜트의 관측된 상태(환경)를

최고의 액추에이터 명령(행동)으로 매핑하는

제어기(정책)를 설계하는 방법을 찾고 있습니다.

제어기를 설계할 때는 기본적으로

일회성 정책 업데이트를 시행하는 셈이죠.

최적의 제어기를 설계하는 한 가지 방법은

LQR에서 하는 것과 같이 비용 함수를 최소화하는 겁니다.

여기서 비용은 음의 보상에 해당하므로,

보상을 최대화하면 비용을 최소화하는 문제가 해결됩니다.

차이점이 있다면 강화 학습에서는 설계자가

명시적으로 문제를 해결하게 하기보다는

컴퓨터가 최적의 동작을 학습하려고 노력한다는 것입니다.

이는 각 샘플 시간에 매개 변수를 조정하는

적응형 제어기의 조정 메커니즘과 같습니다.

이러한 방식으로 시스템 자체에 대해

전혀 알지 못하는 상태에서도 정책,

즉 제어기를 설계할 수 있는 것이죠.

기존의 제어 문제를 하나도 해결할 필요 없이,

아주 근사한 '시행착오'라는 프로세스를 통해

컴퓨터가 스스로 올바른 매개 변수를 학습하게 할 수 있습니다.

아무리 학습 알고리즘이

우리 대신 대부분의 일을 수행한다고 해도,

완전히 무지한 채로

이 프로세스를 수행할 수는 없습니다.

시작하기 전에 몇 가지 알아야 할 게 있죠.

우선 우리가 제어하려고 하는 시스템을 이해한 다음

문제를 전통적인 제어 기법으로 해결하는 것이

더 나은지, 아니면 강화 학습으로

해결하는 것이 더 나은지 판단해야 합니다.

학습 경로를 선택한 다음에는 정책을 설정해야 합니다.

그래야 충분한 매개 변수와 올바른 구조체를 가질 수 있고,

성공적인 조정도 가능하기 때문입니다.

다차원 시스템을 제어하고 싶은데

단일 매개 변수만 제공한다면

아무런 도움이 되지 않습니다.

또한 성공적인 결과가 무엇일지도 알아야 하고,

제어기가 잘하는 경우 보상도 해야 합니다.

이를 위해서는 보상 함수를 만들어야 하죠.

그래야 학습 알고리즘이 언제 더

나아지는지를 이해할 수 있으며 궁극적으로

여러분이 실제로 원하는 결과에 도달할 수 있습니다.

셋째로 보상과 시스템 상태를 볼 수 있고

매개 변수를 조정하는 방법을 아는

효율적인 알고리즘을 적용해야만 프로세스는

합리적인 시간 안에 수렴할 수 있습니다.

여기서 탐색/이용에 대한 매개 변수 및

미래의 보상 할인도 설정해야 하죠.

이어지는 몇 개의 동영상에서는 강화 학습의

워크플로를 보다 자세히 살펴보며

이 모든 내용을 확장해 보도록 하겠습니다.

정책의 구조체를 살펴보고 신경망에 대해서도 소개하겠습니다.

적절한 보상 함수를 만드는 경우

최종 결과에 미치는 영향에 대해서도 다루고,

더불어 몇 가지 흥미로운 학습 알고리즘도

개략적으로 살펴보겠습니다.

다음 Tech Talk 동영상을 놓치고 싶지 않으시다면

이 채널을 구독해 주시기 바랍니다.

또한 제어 관련 내용은 제 채널의 제어 시스템 강좌에서

자세히 다루고 있으니 확인해 보시기 바랍니다.

시청해 주셔서 감사합니다.

다음 동영상에서 뵙겠습니다.

